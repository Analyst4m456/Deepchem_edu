{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ec19ca-81bd-42bb-aab2-be9e34430009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (C:\\Users\\yyyyx\\miniconda3\\envs\\deepchem\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.feat.graph_data import GraphData\n",
    "from deepchem.feat import MolGraphConvFeaturizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa19daf5-a509-4a8a-bd14-eef7f5ff7156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: numpy, tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28833125-d217-45d5-88ec-5affddc83970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./AqsolDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6869c4-9ac3-4b17-8aa4-dbdf0bb8c77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Drug_ID</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>N,N,N-trimethyloctadecan-1-aminium bromide</td>\n",
       "      <td>CCCCCCCCCCCCCCCCCC[N+](C)(C)C.[Br-]</td>\n",
       "      <td>-3.616127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Benzo[cd]indol-2(1H)-one</td>\n",
       "      <td>O=C1Nc2cccc3cccc1c23</td>\n",
       "      <td>-3.254767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4-chlorobenzaldehyde</td>\n",
       "      <td>O=Cc1ccc(Cl)cc1</td>\n",
       "      <td>-2.177078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>zinc bis[2-hydroxy-3,5-bis(1-phenylethyl)benzo...</td>\n",
       "      <td>CC(c1ccccc1)c1cc(C(=O)[O-])c(O)c(C(C)c2ccccc2)...</td>\n",
       "      <td>-3.924409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4-({4-[bis(oxiran-2-ylmethyl)amino]phenyl}meth...</td>\n",
       "      <td>c1cc(N(CC2CO2)CC2CO2)ccc1Cc1ccc(N(CC2CO2)CC2CO...</td>\n",
       "      <td>-4.662065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>9977</td>\n",
       "      <td>tetracaine</td>\n",
       "      <td>CCCCNc1ccc(C(=O)OCCN(C)C)cc1</td>\n",
       "      <td>-3.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>9978</td>\n",
       "      <td>tetracycline</td>\n",
       "      <td>CN(C)[C@@H]1C(O)=C(C(N)=O)C(=O)[C@@]2(O)C(O)=C...</td>\n",
       "      <td>-2.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>9979</td>\n",
       "      <td>thymol</td>\n",
       "      <td>Cc1ccc(C(C)C)c(O)c1</td>\n",
       "      <td>-2.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>9980</td>\n",
       "      <td>verapamil</td>\n",
       "      <td>COc1ccc(CCN(C)CCCC(C#N)(c2ccc(OC)c(OC)c2)C(C)C...</td>\n",
       "      <td>-3.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>9981</td>\n",
       "      <td>warfarin</td>\n",
       "      <td>CC(=O)CC(c1ccccc1)c1c(O)c2ccccc2oc1=O</td>\n",
       "      <td>-4.780000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9982 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            Drug_ID  \\\n",
       "0              0         N,N,N-trimethyloctadecan-1-aminium bromide   \n",
       "1              1                           Benzo[cd]indol-2(1H)-one   \n",
       "2              2                               4-chlorobenzaldehyde   \n",
       "3              3  zinc bis[2-hydroxy-3,5-bis(1-phenylethyl)benzo...   \n",
       "4              4  4-({4-[bis(oxiran-2-ylmethyl)amino]phenyl}meth...   \n",
       "...          ...                                                ...   \n",
       "9977        9977                                         tetracaine   \n",
       "9978        9978                                       tetracycline   \n",
       "9979        9979                                             thymol   \n",
       "9980        9980                                          verapamil   \n",
       "9981        9981                                           warfarin   \n",
       "\n",
       "                                                   Drug         Y  \n",
       "0                   CCCCCCCCCCCCCCCCCC[N+](C)(C)C.[Br-] -3.616127  \n",
       "1                                  O=C1Nc2cccc3cccc1c23 -3.254767  \n",
       "2                                       O=Cc1ccc(Cl)cc1 -2.177078  \n",
       "3     CC(c1ccccc1)c1cc(C(=O)[O-])c(O)c(C(C)c2ccccc2)... -3.924409  \n",
       "4     c1cc(N(CC2CO2)CC2CO2)ccc1Cc1ccc(N(CC2CO2)CC2CO... -4.662065  \n",
       "...                                                 ...       ...  \n",
       "9977                       CCCCNc1ccc(C(=O)OCCN(C)C)cc1 -3.010000  \n",
       "9978  CN(C)[C@@H]1C(O)=C(C(N)=O)C(=O)[C@@]2(O)C(O)=C... -2.930000  \n",
       "9979                                Cc1ccc(C(C)C)c(O)c1 -2.190000  \n",
       "9980  COc1ccc(CCN(C)CCCC(C#N)(c2ccc(OC)c(OC)c2)C(C)C... -3.980000  \n",
       "9981              CC(=O)CC(c1ccccc1)c1c(O)c2ccccc2oc1=O -4.780000  \n",
       "\n",
       "[9982 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20454eb-293b-4638-a605-9070ef24e9d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to featurize datapoint 38, [Mo]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 143, [Mg+2]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 199, [Cd+2]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 296, [Re]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 419, N. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 801, [As]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 827, [Gd]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 885, [Pb]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 1148, [Te]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 1171, [Y]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 1245, [Cu]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 1290, [Hg]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 1440, S. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 1533, [Zr]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 1668, [Hf]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 1987, [V]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 2055, [Sr]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 2415, [Cs]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 2631, [La]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3041, [Mn]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3053, [Ta]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3082, [Br-]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3385, [Cr]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3399, [P]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3425, [Dy]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3433, [Co]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3491, [Se]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3504, [Fe]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3507, [B]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3522, [Nd]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3558, [Ir]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3567, [Si]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3592, [Sn]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3640, [Nb]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3648, [Be]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3650, [Au]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 3755, C. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "Failed to featurize datapoint 4792, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(class RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\n",
      "Failed to featurize datapoint 5043, None. Appending empty array\n",
      "Exception message: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\n",
      "did not match C++ signature:\n",
      "    CanonicalRankAtoms(class RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\n",
      "Exception message: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (9982,) + inhomogeneous part.\n"
     ]
    }
   ],
   "source": [
    "# 1. SMILES에서 그래프 형식으로 변환\n",
    "featurizer = MolGraphConvFeaturizer()\n",
    "features = featurizer.featurize(df[\"Drug\"])  # Smiles 컬럼에서 특징 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2149f8d1-d38c-4427-be89-f5d0fa447702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([GraphData(node_features=[23, 30], edge_index=[2, 42], edge_features=None),\n",
       "       GraphData(node_features=[13, 30], edge_index=[2, 30], edge_features=None),\n",
       "       GraphData(node_features=[9, 30], edge_index=[2, 18], edge_features=None),\n",
       "       ...,\n",
       "       GraphData(node_features=[11, 30], edge_index=[2, 22], edge_features=None),\n",
       "       GraphData(node_features=[33, 30], edge_index=[2, 68], edge_features=None),\n",
       "       GraphData(node_features=[23, 30], edge_index=[2, 50], edge_features=None)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63176f01-8bbb-495a-98c4-7812ccef7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 레이블 설정 (pIC50)\n",
    "labels = df[\"Y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b7b752-7882-4403-aba3-ed30ab465c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 가중치 (필요시 기본값으로 1 설정)\n",
    "weights = None  # 기본적으로 None으로 설정. 커스텀 가중치가 있으면 지정.\n",
    "\n",
    "# 4. 데이터셋 생성\n",
    "dataset = dc.data.NumpyDataset(X=features, y=labels, w=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb933fa-9bf4-4248-9614-81c44a85a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid indices: [38, 143, 199, 296, 419, 801, 827, 885, 1148, 1171, 1245, 1290, 1440, 1533, 1668, 1987, 2055, 2415, 2631, 3041, 3053, 3082, 3385, 3399, 3425, 3433, 3491, 3504, 3507, 3522, 3558, 3567, 3592, 3640, 3648, 3650, 3755, 4792, 5043]\n"
     ]
    }
   ],
   "source": [
    "invalid_indices = []\n",
    "for i, x_item in enumerate(dataset.X):\n",
    "    # 예: x_item 이 비어 있거나, 그래프가 아닌 ndarray인 경우 invalid 처리\n",
    "    if (isinstance(x_item, np.ndarray) and len(x_item) == 0):\n",
    "        invalid_indices.append(i)\n",
    "\n",
    "print(\"Invalid indices:\", invalid_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a6082ef-6240-4430-849f-7d5ae3f6e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "\n",
    "# 유효한 인덱스를 골라낸다\n",
    "valid_indices = [\n",
    "    i for i in range(len(dataset.X)) if i not in invalid_indices\n",
    "]\n",
    "\n",
    "# 유효한 X, y, w, ids만 추출\n",
    "filtered_X   = [dataset.X[i] for i in valid_indices]\n",
    "filtered_y   = [dataset.y[i] for i in valid_indices]\n",
    "filtered_w   = [dataset.w[i] for i in valid_indices]\n",
    "filtered_ids = [dataset.ids[i] for i in valid_indices]\n",
    "\n",
    "# 새롭게 NumpyDataset 생성\n",
    "new_dataset = dc.data.NumpyDataset(\n",
    "    X=filtered_X,\n",
    "    y=filtered_y,\n",
    "    w=filtered_w,\n",
    "    ids=filtered_ids\n",
    ")\n",
    "\n",
    "# 이제 new_test_dataset을 대신 사용하면 됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb67662-97a4-40e6-abc5-d61da1738bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (9982,), y.shape: (9982,), w.shape: (9982,), task_names: [0]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356fa1aa-2171-48b1-b039-86fe5b8b5eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (9943,), y.shape: (9943,), w.shape: (9943,), task_names: [0]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21870a5c-2b14-40d9-8e35-ae908f24886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 9943\n",
      "Feature shape: 23 nodes, 42 edges\n",
      "First label: -3.6161271205\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 확인\n",
    "print(f\"Number of samples in dataset: {len(new_dataset)}\")\n",
    "print(f\"Feature shape: {new_dataset.X[0].node_features.shape[0]} nodes, {new_dataset.X[0].edge_index.shape[1]} edges\")\n",
    "print(f\"First label: {new_dataset.y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c4477a-cb2c-4046-be81-4540604a190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7954\n",
      "Validation dataset size: 994\n",
      "Test dataset size: 995\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 Train/Validation/Test로 나누기\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(new_dataset)\n",
    "\n",
    "# 확인\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be1393b-418f-4a5d-8407-58b08428d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCNModel 초기화\n",
    "model = dc.models.GCNModel(\n",
    "    n_tasks=1,\n",
    "    graph_conv_layers=[64, 64],\n",
    "    dense_layer_size=128,\n",
    "    dropout=0.2,\n",
    "    mode='regression',\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c5dd870-9c2e-41b4-841c-8c00056c9b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7832112884521485"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d030e4-d50e-4053-83cd-453a10ec482b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7810150146484375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4c1f9a-561f-49d8-9aa3-867d541cc59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd3f45b9-ae88-4422-a0a6-b69cb11efca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "413fd9fd-c74a-41e5-b4a4-05f08e30ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# 간단한 연산으로 디바이스 확인\n",
    "a = tf.constant([1.0, 2.0, 3.0])\n",
    "b = tf.constant([4.0, 5.0, 6.0])\n",
    "c = a + b\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "889efbd1-97de-4aa3-8b98-b2d9ba99d3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   0%|                                                                     | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running combination 1/36:\n",
      "graph_conv_layers: [64, 64], dense_layer_size: 128, dropout: 0.2, learning_rate: 0.001, batch_size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   0%|                                                                     | 0/36 [03:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# 평가\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     train_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(train_dataset, [dc\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMetric(dc\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mpearson_r2_score)])\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:338\u001b[0m, in \u001b[0;36mTorchModel.fit\u001b[1;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    290\u001b[0m         dataset: Dataset,\n\u001b[0;32m    291\u001b[0m         nb_epoch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m         callbacks: Union[Callable, List[Callable]] \u001b[38;5;241m=\u001b[39m [],\n\u001b[0;32m    299\u001b[0m         all_losses: Optional[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train this model on a dataset.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    The average loss over the most recent checkpoint interval\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_checkpoints_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_losses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:433\u001b[0m, in \u001b[0;36mTorchModel.fit_generator\u001b[1;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    430\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    432\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 433\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [outputs]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\deepchem\\models\\torch_models\\gcn.py:187\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, g)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict graph labels\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    logits for classes before softmax.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    186\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfeat_name]\n\u001b[1;32m--> 187\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tasks \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\dgllife\\model\\model_zoo\\gcn_predictor.py:120\u001b[0m, in \u001b[0;36mGCNPredictor.forward\u001b[1;34m(self, bg, feats)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, bg, feats):\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Graph-level regression/soft classification.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m        * B for the number of graphs in the batch\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     graph_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout(bg, node_feats)\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(graph_feats)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\dgllife\\model\\gnn\\gcn.py:235\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, g, feats)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update node representations.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m      hidden_sizes[-1] in initialization.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gnn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_layers:\n\u001b[1;32m--> 235\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feats\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\dgllife\\model\\gnn\\gcn.py:103\u001b[0m, in \u001b[0;36mGCNLayer.forward\u001b[1;34m(self, g, feats)\u001b[0m\n\u001b[0;32m    101\u001b[0m new_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_conv(g, feats)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m--> 103\u001b[0m     res_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     new_feats \u001b[38;5;241m=\u001b[39m new_feats \u001b[38;5;241m+\u001b[39m res_feats\n\u001b[0;32m    105\u001b[0m new_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(new_feats)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem\\lib\\site-packages\\torch\\nn\\functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "\n",
    "# 하이퍼파라미터 목록 정의\n",
    "param_grid = {\n",
    "    'graph_conv_layers': [[64, 64], [128, 128], [64, 128, 64]],\n",
    "    'dense_layer_size': [128, 256],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'learning_rate': [1e-3, 5e-4, 1e-4],\n",
    "    'batch_size': [128]\n",
    "}\n",
    "\n",
    "# 결과를 저장할 데이터프레임 초기화\n",
    "results = []\n",
    "\n",
    "# 하이퍼파라미터 조합 반복\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "combinations = list(product(\n",
    "    param_grid['graph_conv_layers'],\n",
    "    param_grid['dense_layer_size'],\n",
    "    param_grid['dropout'],\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['batch_size']\n",
    "))\n",
    "\n",
    "for i, (graph_conv_layers, dense_layer_size, dropout, learning_rate, batch_size) in enumerate(tqdm(combinations, desc=\"Grid Search Progress\")):\n",
    "    \n",
    "    print(f\"\\nRunning combination {i+1}/{len(combinations)}:\")\n",
    "    print(f\"graph_conv_layers: {graph_conv_layers}, dense_layer_size: {dense_layer_size}, dropout: {dropout}, learning_rate: {learning_rate}, batch_size: {batch_size}\")\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = dc.models.GCNModel(\n",
    "        n_tasks=1,\n",
    "        graph_conv_layers=graph_conv_layers,\n",
    "        dense_layer_size=dense_layer_size,\n",
    "        dropout=dropout,\n",
    "        mode='regression',\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # 모델 학습\n",
    "    try:\n",
    "        model.fit(train_dataset, nb_epoch=100)\n",
    "\n",
    "        # 평가\n",
    "        train_score = model.evaluate(train_dataset, [dc.metrics.Metric(dc.metrics.pearson_r2_score)])\n",
    "        valid_score = model.evaluate(valid_dataset, [dc.metrics.Metric(dc.metrics.pearson_r2_score)])\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append({\n",
    "            'graph_conv_layers': graph_conv_layers,\n",
    "            'dense_layer_size': dense_layer_size,\n",
    "            'dropout': dropout,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'train_r2_score': train_score['pearson_r2_score'],\n",
    "            'valid_r2_score': valid_score['pearson_r2_score']\n",
    "        })\n",
    "\n",
    "        print(f\"Train R2 Score: {train_score['pearson_r2_score']}, Validation R2 Score: {valid_score['pearson_r2_score']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과 출력 및 저장\n",
    "results_df = results_df.sort_values(by='valid_r2_score', ascending=False)\n",
    "results_df.to_csv(\"gridsearch_results.csv\", index=False)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\nTop Results:\")\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06dcad00-ab99-47ce-9c8c-9119cce54837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "import tensorflow as tf\n",
    "\n",
    "# GPU 메모리를 최대한 활용하도록 설정\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], False)  # 동적 메모리 할당 비활성화\n",
    "        print(\"GPU set to use maximum memory\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"RuntimeError: {e}\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "422ea6aa-f2ee-4577-bacb-2e7208b23466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f919c-0163-432a-9cf9-4de608a6f881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   0%|                                                                     | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running combination 1/36:\n",
      "graph_conv_layers: [64, 64], dense_layer_size: 128, dropout: 0.2, learning_rate: 0.001, batch_size: 256\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 목록 정의\n",
    "param_grid = {\n",
    "    'graph_conv_layers': [[64, 64], [128, 128], [64, 128, 64]],\n",
    "    'dense_layer_size': [128, 256],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'learning_rate': [1e-3, 5e-4, 1e-4],\n",
    "    'batch_size': [256]  # GPU 메모리 상황에 따라 다양한 배치 크기를 실험\n",
    "}\n",
    "\n",
    "# 결과를 저장할 데이터프레임 초기화\n",
    "results = []\n",
    "\n",
    "# 하이퍼파라미터 조합 반복\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "combinations = list(product(\n",
    "    param_grid['graph_conv_layers'],\n",
    "    param_grid['dense_layer_size'],\n",
    "    param_grid['dropout'],\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['batch_size']\n",
    "))\n",
    "\n",
    "for i, (graph_conv_layers, dense_layer_size, dropout, learning_rate, batch_size) in enumerate(tqdm(combinations, desc=\"Grid Search Progress\")):\n",
    "    \n",
    "    print(f\"\\nRunning combination {i+1}/{len(combinations)}:\")\n",
    "    print(f\"graph_conv_layers: {graph_conv_layers}, dense_layer_size: {dense_layer_size}, dropout: {dropout}, learning_rate: {learning_rate}, batch_size: {batch_size}\")\n",
    "    \n",
    "    # 모델 초기화\n",
    "    with tf.device('/GPU:0'):  # 명시적으로 GPU 사용 설정\n",
    "        model = dc.models.GCNModel(\n",
    "            n_tasks=1,\n",
    "            graph_conv_layers=graph_conv_layers,\n",
    "            dense_layer_size=dense_layer_size,\n",
    "            dropout=dropout,\n",
    "            mode='regression',\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "    # 모델 학습\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            model.fit(train_dataset, nb_epoch=100)\n",
    "\n",
    "        # 평가\n",
    "        train_score = model.evaluate(train_dataset, [dc.metrics.Metric(dc.metrics.pearson_r2_score)])\n",
    "        valid_score = model.evaluate(valid_dataset, [dc.metrics.Metric(dc.metrics.pearson_r2_score)])\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append({\n",
    "            'graph_conv_layers': graph_conv_layers,\n",
    "            'dense_layer_size': dense_layer_size,\n",
    "            'dropout': dropout,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'train_r2_score': train_score['pearson_r2_score'],\n",
    "            'valid_r2_score': valid_score['pearson_r2_score']\n",
    "        })\n",
    "\n",
    "        print(f\"Train R2 Score: {train_score['pearson_r2_score']}, Validation R2 Score: {valid_score['pearson_r2_score']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과 출력 및 저장\n",
    "results_df = results_df.sort_values(by='valid_r2_score', ascending=False)\n",
    "results_df.to_csv(\"gridsearch_results.csv\", index=False)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\nTop Results:\")\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee982d9a-a5a7-401a-9fb6-4d54d846c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'pearson_r2_score': 0.8728071088137395}\n",
      "Test set score: {'pearson_r2_score': 0.7761037587217018}\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "print(\"Training set score:\", model.evaluate(train_dataset, [metric]))\n",
    "print(\"Test set score:\", model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a35b1fe-f187-4f38-b6b0-7a41303983ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: {'pearson_r2_score': 0.8320443628967614}\n",
      "Validation score: {'pearson_r2_score': 0.802325296546151}\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가 / GPT 파라미터 버전\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "train_score = model.evaluate(train_dataset, [metric])\n",
    "valid_score = model.evaluate(valid_dataset, [metric])\n",
    "\n",
    "print(\"Train score:\", train_score)\n",
    "print(\"Validation score:\", valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3ab43f3-71ff-4384-8ee9-80a1eb9298d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2 Score: 0.735293292904228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Test 데이터셋에 대한 예측값 추론\n",
    "y_pred = model.predict(test_dataset)\n",
    "\n",
    "# 2. 실제 y값 (test_dataset.y) 과 비교하여 R2 스코어 계산\n",
    "test_r2 = r2_score(test_dataset.y, y_pred)\n",
    "\n",
    "print(\"Test R2 Score:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab73bc82-aba1-495d-a59b-576ff82573a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2 Score: 0.7949052245292128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Test 데이터셋에 대한 예측값 추론\n",
    "y_pred = model.predict(test_dataset)\n",
    "\n",
    "# 2. 실제 y값 (test_dataset.y) 과 비교하여 R2 스코어 계산\n",
    "test_r2 = r2_score(test_dataset.y, y_pred)\n",
    "\n",
    "print(\"Test R2 Score:\", test_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3971682-2f05-4f31-b250-c86f4a531b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 후\n",
    "model.save_checkpoint(model_dir=\"./Aquasol\")  # 원하는 경로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "913b0cb0-59c2-438c-b4f9-b2f72543ec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deepchem.models.torch_models.gcn.GCNModel at 0x1d6a1a96760>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e3696-b523-4726-999c-4ec8039493eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "deepchem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

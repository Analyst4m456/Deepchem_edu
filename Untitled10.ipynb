{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02948926-e75b-403e-b976-7f8249cfd43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (C:\\Users\\yyyyx\\miniconda3\\envs\\deepchem\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.feat import MolGraphConvFeaturizer\n",
    "from deepchem.data import CSVLoader\n",
    "from deepchem.splits import RandomSplitter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib as jb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from deepchem.models.layers import GraphConv, GraphPool, GraphGather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af15e1a9-e3c4-4d4d-8173-11e9dfdab027",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = jb.load('./Aquasol_dataset_MolGraphConv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbe0359-32c8-4ef3-a6c1-450bc902f5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (9943,), y.shape: (9943,), w.shape: (9943,), task_names: [0]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e499fd10-965b-4495-a3a8-aa63a535d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 스플릿 (train/valid/test)\n",
    "splitter = RandomSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c629b0dd-f63c-429d-ad7f-732d95c54eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 9943\n",
      "Train dataset size: 7954\n",
      "Validation dataset size: 994\n",
      "Test dataset size: 995\n"
     ]
    }
   ],
   "source": [
    "# 확인\n",
    "print(f'Original dataset size: {len(new_dataset)}')\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee103b9-0907-439d-b7ba-f73f42b2d2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([GraphData(node_features=[14, 30], edge_index=[2, 32], edge_features=None),\n",
       "       GraphData(node_features=[15, 30], edge_index=[2, 32], edge_features=None),\n",
       "       GraphData(node_features=[17, 30], edge_index=[2, 34], edge_features=None),\n",
       "       ...,\n",
       "       GraphData(node_features=[40, 30], edge_index=[2, 80], edge_features=None),\n",
       "       GraphData(node_features=[12, 30], edge_index=[2, 24], edge_features=None),\n",
       "       GraphData(node_features=[12, 30], edge_index=[2, 24], edge_features=None)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89c3fe62-760a-4226-a88b-4e1bc38e7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2. PyTorch GCN 모델 정의\n",
    "###############################################################################\n",
    "class GCNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepChem의 그래프 컨볼루션 레이어를 이용하여 GCN을 구성하는 예시 모델입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=64, output_dim=1):\n",
    "        super(GCNModel, self).__init__()\n",
    "        # DeepChem에서 제공하는 그래프 컨볼루션 레이어(ConvMolLayer, GraphConv 등)가 있습니다.\n",
    "        # 여기서는 GraphConv 레이어를 예시로 사용합니다.\n",
    "        self.gc1 = GraphConv(hidden_dim)\n",
    "        self.gc2 = GraphConv(hidden_dim)\n",
    "        \n",
    "        # Readout 전에 feature를 조금 더 변환할 수도 있음\n",
    "        self.readout = GraphPool()  # 기본적으로 sum pooling\n",
    "        \n",
    "        # 최종 예측 레이어\n",
    "        self.dense = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: (node_features, adjacency_list, num_nodes)의 튜플 형태로 들어옵니다.\n",
    "          - node_features: shape (batch_size, max_atoms, n_feat)\n",
    "          - adjacency_list: (batch_size, max_bonds, 2)\n",
    "          - num_nodes: (batch_size,)\n",
    "        \"\"\"\n",
    "        node_features, adjacency_list, num_nodes = inputs\n",
    "        \n",
    "        # GraphConv 레이어 통과\n",
    "        h = self.gc1(node_features, adjacency_list, num_nodes)\n",
    "        h = F.relu(h)\n",
    "        h = self.gc2(h, adjacency_list, num_nodes)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        # Readout(여기서는 Sum Pooling)\n",
    "        h = self.readout(h, num_nodes)  # shape (batch_size, hidden_dim)\n",
    "        \n",
    "        # 회귀를 위한 최종 Dense\n",
    "        out = self.dense(h)  # shape (batch_size, output_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaaa2193-691f-432a-bcba-7280fc260eea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mdc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphConvModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# 예측해야 하는 타겟 개수 (회귀 문제에서 1개라면 1)\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mregression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 회귀냐 분류냐\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_dataset, nb_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem_2\\lib\\site-packages\\deepchem\\models\\graph_models.py:959\u001b[0m, in \u001b[0;36mGraphConvModel.__init__\u001b[1;34m(self, n_tasks, graph_conv_layers, dense_layer_size, dropout, mode, number_atom_features, n_classes, batch_size, batch_normalize, uncertainty, **kwargs)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muncertainty \u001b[38;5;241m=\u001b[39m uncertainty\n\u001b[1;32m--> 959\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_GraphConvKerasModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_tasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mgraph_conv_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_conv_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdense_layer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_layer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mnumber_atom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber_atom_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbatch_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muncertainty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muncertainty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    970\u001b[0m   output_types \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem_2\\lib\\site-packages\\deepchem\\models\\graph_models.py:803\u001b[0m, in \u001b[0;36m_GraphConvKerasModel.__init__\u001b[1;34m(self, n_tasks, graph_conv_layers, dense_layer_size, dropout, mode, number_atom_features, n_classes, batch_normalize, uncertainty, batch_size)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    784\u001b[0m              n_tasks,\n\u001b[0;32m    785\u001b[0m              graph_conv_layers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    792\u001b[0m              uncertainty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    793\u001b[0m              batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m    794\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"An internal keras model class.\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \n\u001b[0;32m    796\u001b[0m \u001b[38;5;124;03m  The graph convolutions use a nonstandard control flow so the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;124;03m  All arguments have the same meaning as in GraphConvModel.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 803\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_GraphConvKerasModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode must be either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem_2\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem_2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem_2\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:622\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    618\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetRunEagerOpAsFunction(\n\u001b[0;32m    619\u001b[0m       opts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_eager_op_as_function)\n\u001b[0;32m    620\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetJitCompileRewrite(\n\u001b[0;32m    621\u001b[0m       opts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile_rewrite)\n\u001b[1;32m--> 622\u001b[0m   context_handle \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_NewContext(opts)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_DeleteContextOptions(opts)\n",
      "\u001b[1;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found."
     ]
    }
   ],
   "source": [
    "model = dc.models.GraphConvModel(\n",
    "    n_tasks=1,            # 예측해야 하는 타겟 개수 (회귀 문제에서 1개라면 1)\n",
    "    mode='regression',    # 회귀냐 분류냐\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1527d5ce-850f-4282-8a87-997f27ab0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCNModel 초기화\n",
    "model = dc.models.GCNModel(\n",
    "    n_tasks=1,\n",
    "    graph_conv_layers=[64, 64],\n",
    "    dense_layer_size=128,\n",
    "    dropout=0.2,\n",
    "    mode='regression',\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc231c4d-71b6-4860-9629-e60277a63fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 3s\n",
      "Wall time: 45.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3576590220133464"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ff74f53-7f0b-4278-95e1-978197001a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 13\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m dc\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mTorchModel(\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mGCNModel(hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# 위에서 정의한 PyTorch 모델\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     loss\u001b[38;5;241m=\u001b[39mdc\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mL2Loss(),               \u001b[38;5;66;03m# 회귀이므로 L2Loss 사용\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     output_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]                   \u001b[38;5;66;03m# 예측의 형태\u001b[39;00m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# nb_epoch, batch_size 등은 상황에 맞게 조절\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem_2\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:330\u001b[0m, in \u001b[0;36mTorchModel.fit\u001b[1;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    282\u001b[0m         dataset: Dataset,\n\u001b[0;32m    283\u001b[0m         nb_epoch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    290\u001b[0m         callbacks: Union[Callable, List[Callable]] \u001b[38;5;241m=\u001b[39m [],\n\u001b[0;32m    291\u001b[0m         all_losses: Optional[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Train this model on a dataset.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m  Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m  The average loss over the most recent checkpoint interval\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m \"\"\"\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_checkpoints_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_losses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem_2\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:413\u001b[0m, in \u001b[0;36mTorchModel.fit_generator\u001b[1;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    411\u001b[0m   restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    412\u001b[0m inputs: OneOrMany[torch\u001b[38;5;241m.\u001b[39mTensor]\n\u001b[1;32m--> 413\u001b[0m inputs, labels, weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Execute the loss function, accumulating the gradients.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem_2\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:901\u001b[0m, in \u001b[0;36mTorchModel._prepare_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    897\u001b[0m inputs, labels, weights \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    898\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    899\u001b[0m     x\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m    900\u001b[0m ]\n\u001b[1;32m--> 901\u001b[0m input_tensors \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mas_tensor(x, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    903\u001b[0m   labels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    904\u001b[0m       x\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m labels\n\u001b[0;32m    905\u001b[0m   ]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\deepchem_2\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:901\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    897\u001b[0m inputs, labels, weights \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    898\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    899\u001b[0m     x\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m    900\u001b[0m ]\n\u001b[1;32m--> 901\u001b[0m input_tensors \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    903\u001b[0m   labels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    904\u001b[0m       x\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m labels\n\u001b[0;32m    905\u001b[0m   ]\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 3. DeepChem의 TorchModel로 래핑 & 학습\n",
    "###############################################################################\n",
    "# 모델을 PyTorch로 정의했으므로, DeepChem의 TorchModel을 사용합니다.\n",
    "model = dc.models.TorchModel(\n",
    "    model=GCNModel(hidden_dim=64, output_dim=1),  # 위에서 정의한 PyTorch 모델\n",
    "    loss=dc.models.losses.L2Loss(),               # 회귀이므로 L2Loss 사용\n",
    "    output_types=[\"prediction\"]                   # 예측의 형태\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "# nb_epoch, batch_size 등은 상황에 맞게 조절\n",
    "model.fit(train_dataset, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17276f0e-7496-4889-884c-bec4a8242332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2:  {'pearson_r2_score': 0.776267570646285}\n",
      "Valid R^2:  {'pearson_r2_score': 0.7438408257849691}\n",
      "Test  R^2:  {'pearson_r2_score': 0.7343115565439987}\n",
      "Train MAE:  {'mean_absolute_error': 0.8807286419332556}\n",
      "Valid MAE:  {'mean_absolute_error': 0.9010264118836565}\n",
      "Test  MAE:  {'mean_absolute_error': 0.9475775182251481}\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 4. 모델 평가\n",
    "###############################################################################\n",
    "# 평가지표 설정 (예: R^2 스코어, MAE, RMSE 등)\n",
    "metric_r2 = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "metric_mae = dc.metrics.Metric(dc.metrics.mean_absolute_error)\n",
    "\n",
    "print(\"Train R^2: \", model.evaluate(train_dataset, [metric_r2]))\n",
    "print(\"Valid R^2: \", model.evaluate(valid_dataset, [metric_r2]))\n",
    "print(\"Test  R^2: \", model.evaluate(test_dataset, [metric_r2]))\n",
    "\n",
    "print(\"Train MAE: \", model.evaluate(train_dataset, [metric_mae]))\n",
    "print(\"Valid MAE: \", model.evaluate(valid_dataset, [metric_mae]))\n",
    "print(\"Test  MAE: \", model.evaluate(test_dataset, [metric_mae]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b1ff7-2d8c-4695-be46-958bf481e89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "deepchem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
